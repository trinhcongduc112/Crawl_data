"""
Re-scrape Developer Guide t·ª´ docs.abivin.com
ƒê√¢y l√† script ƒë·ªÉ fetch l·∫°i full content cho c√°c API documentation pages
"""

import os
import json
import re
import time
import sys
import requests
from bs4 import BeautifulSoup
from typing import Dict, List, Tuple

# Fix encoding for Windows console
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

DEVELOPER_GUIDE_DIR = "developer_guide/content_developer_guide"
URL_MAPPING_FILE = "developer_guide/url_mapping_developer_guide.json"

def fetch_html(url: str, retries: int = 3) -> str:
    """Fetch HTML v·ªõi retry logic"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            if attempt < retries - 1:
                print(f"  Retry {attempt + 1}/{retries}...")
                time.sleep(2)
            else:
                print(f"  ‚úó Failed after {retries} attempts: {e}")
                return ""
    return ""

def extract_api_content(soup: BeautifulSoup) -> str:
    """Extract API documentation content"""
    # T√¨m ph·∫ßn main content c·ªßa API docs
    content_parts = []
    
    # T√¨m article ho·∫∑c main content area
    article = soup.find('article') or soup.find('main') or soup.find('div', class_=re.compile('content|documentation|api'))
    
    if article:
        # Clean unwanted elements
        for tag in article.find_all(['script', 'style', 'nav', 'aside', 'footer', 'header']):
            tag.decompose()
        
        # T√¨m c√°c section ch√≠nh
        sections = article.find_all(['section', 'div'], class_=re.compile('api|endpoint|method|request|response'))
        
        if sections:
            for section in sections:
                html = str(section)
                # Clean HTML m·ªôt ch√∫t
                html = re.sub(r'\s+', ' ', html)
                content_parts.append(html)
        else:
            # N·∫øu kh√¥ng c√≥ section ri√™ng, l·∫•y to√†n b·ªô content
            html = str(article)
            content_parts.append(html)
    else:
        # Fallback: l·∫•y body content
        body = soup.find('body')
        if body:
            for tag in body.find_all(['script', 'style', 'nav', 'aside', 'footer', 'header']):
                tag.decompose()
            content_parts.append(str(body))
    
    return '\n'.join(content_parts)

def clean_html_for_display(html: str) -> str:
    """Clean HTML ƒë·ªÉ hi·ªÉn th·ªã t·ªët trong Odoo"""
    if not html:
        return "<p>No content available</p>"
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # Lo·∫°i b·ªè c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt
    for tag in soup.find_all(['script', 'style', 'link', 'meta']):
        tag.decompose()
    
    # Lo·∫°i b·ªè c√°c elements tr·ªëng
    for tag in soup.find_all(['div', 'span', 'p']):
        if not tag.get_text(strip=True):
            tag.decompose()
    
    # Th√™m style cho code blocks
    for pre in soup.find_all('pre'):
        pre['style'] = 'background: #f6f8fa; border: 1px solid #eaecef; border-radius: 6px; padding: 12px; overflow: auto;'
    
    for code in soup.find_all('code'):
        if code.parent.name != 'pre':  # Code block ri√™ng l·∫ª
            code['style'] = 'background: #f6f8fa; padding: 2px 6px; border-radius: 3px; font-family: Consolas, monospace;'
    
    # Th√™m style cho images
    for img in soup.find_all('img'):
        img['style'] = 'max-width: 100%; height: auto;'
    
    return str(soup)

def update_developer_guide_file(json_path: str, new_html: str) -> bool:
    """Update JSON file v·ªõi n·ªôi dung m·ªõi"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            doc = json.load(f)
        
        old_html = doc.get('html_content', '')
        
        # N·∫øu content m·ªõi kh√°c v√† c√≥ √Ω nghƒ©a h∆°n
        if new_html and len(new_html) > len(old_html) * 1.5:
            doc['html_content'] = new_html
            doc['updated_at'] = time.strftime('%Y-%m-%dT%H:%M:%SZ')
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(doc, f, ensure_ascii=False, indent=2)
            return True
        return False
    except Exception as e:
        print(f"  ‚úó Error updating file: {e}")
        return False

def main():
    """Main function"""
    print("="*60)
    print("üîÑ RE-SCRAPING DEVELOPER GUIDE")
    print("="*60)
    
    # Load URL mapping
    try:
        with open(URL_MAPPING_FILE, 'r', encoding='utf-8') as f:
            url_mapping = json.load(f)
    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ load URL mapping: {e}")
        return
    
    total = len(url_mapping)
    updated = 0
    failed = 0
    
    for idx, (url, json_path) in enumerate(url_mapping.items(), 1):
        if not json_path.startswith('C:'):
            # Handle relative paths
            json_path = json_path.replace('\\', '/')
        
        # Extract slug ƒë·ªÉ hi·ªÉn th·ªã
        slug = os.path.basename(json_path)
        print(f"\n[{idx}/{total}] {slug}")
        print(f"  URL: {url}")
        
        # Fetch HTML
        html = fetch_html(url)
        if not html:
            print("  ‚úó Kh√¥ng th·ªÉ fetch content")
            failed += 1
            continue
        
        # Parse v√† extract content
        soup = BeautifulSoup(html, 'html.parser')
        api_content = extract_api_content(soup)
        cleaned_content = clean_html_for_display(api_content)
        
        # Update file n·∫øu content t·ªët h∆°n
        if update_developer_guide_file(json_path, cleaned_content):
            print(f"  ‚úì Updated content ({len(cleaned_content)} chars)")
            updated += 1
        else:
            print(f"  ‚Üí Kh√¥ng c·∫ßn update (content kh√¥ng kh√°c nhi·ªÅu)")
        
        # Delay ƒë·ªÉ tr√°nh rate limit
        time.sleep(1)
    
    print("\n" + "="*60)
    print(f"‚úÖ HO√ÄN T·∫§T!")
    print(f"   Total: {total}")
    print(f"   Updated: {updated}")
    print(f"   Failed: {failed}")
    print("="*60)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Ng∆∞·ªùi d√πng h·ªßy b·ªè!")
    except Exception as e:
        print(f"\n‚ùå L·ªñI: {e}")
        import traceback
        traceback.print_exc()






